{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "seed = 0\n",
    "timesteps = list(range(10000))\n",
    "\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# env.seed(seed)\n",
    "env = UnityEnvironment(file_name=\"../Banana_Windows_x86_64/Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "gamma = 0.99\n",
    "eps_start = 1.\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(\"logs/unity/{}\".format(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_STARTED = Events.EPOCH_STARTED\n",
    "EPISODE_COMPLETED = Events.EPOCH_COMPLETED\n",
    "\n",
    "def run_single_timestep(engine, timestep):\n",
    "    eps = engine.state.eps\n",
    "    state = engine.state.current_state\n",
    "    #  (np.int32 because https://github.com/xkiwilabs/DQN-using-PyTorch-and-ML-Agents/issues/2)\n",
    "    action = agent.act(state, eps).astype(np.int32)\n",
    "    \n",
    "#     next_state, reward, done, _ = env.step(action)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]\n",
    "    \n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    \n",
    "    engine.state.current_state = next_state\n",
    "    engine.state.score += reward\n",
    "    \n",
    "    if done:\n",
    "        engine.terminate_epoch()\n",
    "        engine.state.timestep = timestep\n",
    "\n",
    "trainer = Engine(run_single_timestep)\n",
    "\n",
    "@trainer.on(Events.STARTED)\n",
    "def initialize(engine):\n",
    "    # lists containing scores from each episode\n",
    "    engine.state.scores = []                        \n",
    "    engine.state.scores_window = deque(maxlen=100)\n",
    "    engine.state.eps = eps_start\n",
    "\n",
    "\n",
    "@trainer.on(EPISODE_STARTED)\n",
    "def reset_environment_state(engine):\n",
    "#     engine.state.current_state = env.reset()\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    engine.state.current_state = env_info.vector_observations[0]\n",
    "    engine.state.score = 0\n",
    "\n",
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def update_model(engine):\n",
    "    engine.state.eps = max(eps_end, eps_decay*engine.state.eps) # decrease epsilon\n",
    "    \n",
    "    score = engine.state.score\n",
    "    engine.state.scores.append(score)\n",
    "    engine.state.scores_window.append(score)\n",
    "    \n",
    "@trainer.on(EPISODE_COMPLETED(every=10))\n",
    "def log_episode_to_tensorboard(engine):\n",
    "    i = engine.state.epoch\n",
    "#     writer.add_scalar('running reward', engine.state.running_reward, i_episode)\n",
    "    writer.add_scalar('Average episode score', np.mean(engine.state.scores_window), i)\n",
    "    writer.add_scalar('Average environment score', np.mean(engine.state.scores), i)\n",
    "\n",
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def should_finish_training(engine):\n",
    "    if np.mean(engine.state.scores_window)>=13.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(engine.state.epoch, np.mean(engine.state.scores_window)))\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        engine.should_terminate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(timesteps, max_epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
