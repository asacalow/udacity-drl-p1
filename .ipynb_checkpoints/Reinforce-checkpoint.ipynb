{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x283d84540b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 543\n",
    "gamma = 0.99\n",
    "log_interval = 100\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "timesteps = list(range(10000))\n",
    "writer = SummaryWriter('logs/reinforce')\n",
    "\n",
    "\n",
    "def select_action(model, observation):\n",
    "    state = torch.from_numpy(observation).float().unsqueeze(0)\n",
    "    probs = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode(model, optimizer, gamma, eps):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
    "    for log_prob, reward in zip(model.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_log_probs[:]\n",
    "\n",
    "def run_single_timestep(engine, timestep):\n",
    "    observation = engine.state.observation\n",
    "    action = select_action(model, observation)\n",
    "    engine.state.observation, reward, done, _ = env.step(action)\n",
    "#     if args.render:\n",
    "#         env.render()\n",
    "    model.rewards.append(reward)\n",
    "\n",
    "    if done:\n",
    "        engine.terminate_epoch()\n",
    "        engine.state.timestep = timestep\n",
    "\n",
    "trainer = Engine(run_single_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_STARTED = Events.EPOCH_STARTED\n",
    "EPISODE_COMPLETED = Events.EPOCH_COMPLETED\n",
    "\n",
    "@trainer.on(Events.STARTED)\n",
    "def initialize(engine):\n",
    "    engine.state.running_reward = 10\n",
    "\n",
    "@trainer.on(EPISODE_STARTED)\n",
    "def reset_environment_state(engine):\n",
    "    engine.state.observation = env.reset()\n",
    "\n",
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def update_model(engine):\n",
    "    t = engine.state.timestep\n",
    "    engine.state.running_reward = engine.state.running_reward * 0.99 + t * 0.01\n",
    "    finish_episode(model, optimizer, gamma, eps)\n",
    "\n",
    "@trainer.on(EPISODE_COMPLETED(every=log_interval))\n",
    "def log_episode(engine):\n",
    "    i_episode = engine.state.epoch\n",
    "    print(\n",
    "        \"Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\".format(\n",
    "            i_episode, engine.state.timestep, engine.state.running_reward\n",
    "        )\n",
    "    )\n",
    "    \n",
    "@trainer.on(EPISODE_COMPLETED(every=10))\n",
    "def log_episode_to_tensorboard(engine):\n",
    "    i_episode = engine.state.epoch\n",
    "    writer.add_scalar('running reward', engine.state.running_reward, i_episode)\n",
    "\n",
    "@trainer.on(EPISODE_COMPLETED)\n",
    "def should_finish_training(engine):\n",
    "    running_reward = engine.state.running_reward\n",
    "    if running_reward > env.spec.reward_threshold:\n",
    "        print(\n",
    "            \"Solved! Running reward is now {} and \"\n",
    "            \"the last episode runs to {} time steps!\".format(running_reward, engine.state.timestep)\n",
    "        )\n",
    "        engine.should_terminate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tLast length:    75\tAverage length: 47.54\n",
      "Episode 200\tLast length:    31\tAverage length: 95.45\n",
      "Episode 300\tLast length:   199\tAverage length: 144.25\n",
      "Episode 400\tLast length:   199\tAverage length: 178.96\n",
      "Episode 500\tLast length:   199\tAverage length: 182.50\n",
      "Episode 600\tLast length:   199\tAverage length: 191.39\n",
      "Solved! Running reward is now 195.0011948966364 and the last episode runs to 199 time steps!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 106356\n",
       "\tepoch: 664\n",
       "\tepoch_length: 10000\n",
       "\tmax_epochs: 10000\n",
       "\toutput: <class 'NoneType'>\n",
       "\tbatch: 199\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'list'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>\n",
       "\trunning_reward: 195.0011948966364\n",
       "\tobservation: <class 'numpy.ndarray'>\n",
       "\ttimestep: 199"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(timesteps, max_epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-agents",
   "language": "python",
   "name": "ml-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
